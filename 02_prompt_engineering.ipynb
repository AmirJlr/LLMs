{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTTtaDsHvLQiYUkXoTxaFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/LLMs/blob/master/02_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56uiUqdZjol8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_id = \"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"Write a poem about Football(Soccer)\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "H_lrcOCxmPzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Put Prompt in appropriate Template\n",
        "prompt_template = \"[INST]\\nWrite a poem about Football(Soccer)\\n[/INST]\\n\\n\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10, # choose from top 10 probability\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id, # end of sentence\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "WdHyI4QvoM0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add System Prompt\n",
        "system_prompt = \"Answer user questions carefully\"\n",
        "\n",
        "message = \"Tell me about AI\"\n",
        "prompt_template = f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n'\n",
        "prompt_template = prompt_template + f'{message} [/INST]\\n\\n'\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "a5iQKvPapCKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### More advanced system prompt\n",
        "\n",
        "system_prompt = \"Always answer user queries in python and pytorch language carefully. Write codes if needed.\"\n",
        "message = \"write a function to calculate IOU (intersection over union) for two list of points in form of [(x1,y1), (x2,y2),...,(xn,yn)].\"\n",
        "prompt_template = f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n'\n",
        "prompt_template = prompt_template + f'{message} [/INST]\\n\\n'\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=1024,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "ROeUG4otpq3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Good Prompt Has:\n",
        "\n",
        "- ### Input / Context\n",
        "\n",
        "- ### Questions\n",
        "\n",
        "- ### Examples\n",
        "\n",
        "- ### Output Format\n",
        "\n",
        "\n",
        "## More detailed breakdown of effective prompt engineering tips:\n",
        "1. **Clarity and Specificity**:\n",
        "Be explicit: Avoid ambiguity by stating exactly what you want the model to do.\n",
        "Use precise language: Instead of \"summarize in a few sentences,\" specify \"summarize in 2 sentences\".\n",
        "Provide enough context: Help the model understand the topic and desired outcome.\n",
        "\n",
        "2. **Examples and Guidance**:\n",
        "Offer examples: Show the model how you want it to format the output.\n",
        "Use delimiters: Separate different parts of the prompt (e.g., instructions, user input, etc.).\n",
        "Give instructions on what to do, not what not to do: Instead of saying \"don't include personal information,\" say \"include only factual data\".\n",
        "\n",
        "\n",
        "3. **Advanced Techniques**:\n",
        "Chain-of-thought prompting: Encourage the model to explain its reasoning process.\n",
        "Few-shot learning: Provide the model with a few examples of the desired output.\n",
        "Explore different prompts: Experiment with variations to see what works best.\n",
        "\n",
        "\n",
        "4. **Other Important Considerations**:\n",
        "Understand the model's limitations: Know what the model can and cannot do.\n",
        "Be mindful of token limits: Consider the length of your prompts and the model's capacity.\n",
        "Anticipate potential biases and ethical implications: Be aware of the model's potential biases and how they might impact the output.\n",
        "Break down complex tasks: For large tasks, break them into smaller, manageable steps.\n",
        "\n",
        "\n",
        "#### In-context learning (ICL) is a technique where large language models (LLMs) learn a task by providing examples within the prompt, rather than through extensive retraining. Essentially, you \"show\" the model how to do the task, instead of explicitly telling it. This allows LLMs to adapt to new tasks without needing to be fine-tuned on specialized datasets."
      ],
      "metadata": {
        "id": "VygL4Es_qQby"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLBHHeYop7tS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}