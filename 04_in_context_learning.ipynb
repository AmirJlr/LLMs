{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsAVAX1tGjFs1DOQdz8Jot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/LLMs/blob/master/04_in_context_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-context learning (ICL)** allows large language models (LLMs) to perform new tasks without explicitly training them on new data. Instead, they learn from examples included in the prompt itself, like a teacher guiding a student. This approach is task-specific and transient, meaning the model adapts to the examples provided in the prompt only for that particular task.\n",
        "\n",
        "\n",
        "## Here's a breakdown of how in-context learning works:\n",
        "\n",
        "\n",
        "- #### Task-Specific Learning:\n",
        "ICL enables LLMs to learn new tasks on the fly, rather than requiring extensive retraining for each new task.\n",
        "\n",
        "\n",
        "- #### Prompt-Based Learning:\n",
        "The model learns from examples included in the prompt, not from a separate training dataset.\n",
        "\n",
        "\n",
        "- #### Zero-Shot/Few-Shot Learning:\n",
        "ICL is often used in conjunction with zero-shot or few-shot prompting, where the model is provided with examples or instructions within the prompt.\n",
        "\n",
        "\n",
        "- #### Example-Driven Learning:\n",
        "The key is to provide clear and diverse examples that demonstrate the desired task and format within the prompt.\n",
        "\n",
        "\n",
        "- #### Not Explicitly Finetuned:\n",
        "Unlike traditional finetuning where model parameters are updated, ICL relies on the model's pre-trained knowledge and the examples provided in the prompt.\n",
        "\n",
        "\n",
        "## Why In-Context Learning is Important:\n",
        "\n",
        "- #### Flexibility:\n",
        "ICL allows LLMs to adapt to new tasks without requiring extensive retraining, making them more versatile.\n",
        "\n",
        "\n",
        "- #### Cost-Effective:\n",
        "ICL reduces the need for large, labeled datasets, making it a more cost-effective way to train and deploy LLMs.\n",
        "\n",
        "\n",
        "- #### Rapid Prototyping:\n",
        "ICL allows for quick experimentation with different tasks and prompts, enabling rapid prototyping of new applications.\n",
        "\n",
        "\n",
        "## Examples of In-Context Learning:\n",
        "\n",
        "\n",
        "- #### Sentiment Analysis:\n",
        "Providing example sentences and their corresponding sentiment labels in the prompt allows the model to perform sentiment analysis on new text.\n",
        "\n",
        "\n",
        "- #### Question Answering:\n",
        "Including examples of questions and answers based on a specific text allows the model to answer new questions about that text.\n",
        "\n",
        "\n",
        "- #### Translation:\n",
        "Providing examples of translated sentences in the prompt allows the model to translate new sentences.\n",
        "\n",
        "\n",
        "In essence, in-context learning empowers LLMs to learn and perform new tasks by leveraging their pre-trained knowledge and the examples provided in the prompt."
      ],
      "metadata": {
        "id": "P0I77vTAa8CA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfHjWymgXP0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# model_id = \"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Answer exactly in few words.\n",
        "<</SYS>>\n",
        "\n",
        "Answer the question below:\n",
        "\n",
        "Janet's ducks lay 16 eggs per day.\n",
        "She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n",
        "She sells the remainder at the farmers' market daily for $2.5 per fresh duck egg.\n",
        "How much in dollars does she make every day at the farmers' market?\n",
        "\n",
        "Return final results in numeric format.\n",
        "[/INST]\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "zYvSB45JYkFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add `Let's think step by step then return final results in numeric format.` to prompt => Zero-Shot in-context learning"
      ],
      "metadata": {
        "id": "YpNLnxkDae4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Answer exactly in few words.\n",
        "<</SYS>>\n",
        "\n",
        "Answer the question below:\n",
        "\n",
        "Janet's ducks lay 16 eggs per day.\n",
        "She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n",
        "She sells the remainder at the farmers' market daily for $2.5 per fresh duck egg.\n",
        "How much in dollars does she make every day at the farmers' market?\n",
        "\n",
        "Let's think step by step then return final results in numeric format.\n",
        "[/INST]\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt_template,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_length=256,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "VUAegmERY0YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u00VsFbdZZP-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}