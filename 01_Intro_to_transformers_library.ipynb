{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKSqPKJnUq5woBk4MG4g3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/LLMs/blob/master/01_Intro_to_transformers_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KezLl6aMoMH"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "d18_dGGxMxgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple `pipline`"
      ],
      "metadata": {
        "id": "b5XOjqBIWMvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", device='cuda')\n",
        "\n",
        "classifier(\"We are very happy that are madridista!\")"
      ],
      "metadata": {
        "id": "KqanvbMZNHB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence ==> Tokenize ==> Model\n",
        "\n",
        "## Use separate `Tokenizer` and  `Model`"
      ],
      "metadata": {
        "id": "WqVkeWJ7N6Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "57-fwVhxNh1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "CZJVetPLOSLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"We are very happy that are madridista!\"\n",
        "inp = tokenizer(txt, return_tensors=\"pt\") # Pytorch tensor\n",
        "inp"
      ],
      "metadata": {
        "id": "bDKdcHrGO9WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(**inp)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "id": "jRroagEqPWXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Based"
      ],
      "metadata": {
        "id": "MeuTgzQEP5Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "outputs"
      ],
      "metadata": {
        "id": "5gZjKiwSPeW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customize pipeline"
      ],
      "metadata": {
        "id": "gaIB3-SuQs8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "results = classifier([\"We are very happy that are madridista!\",\n",
        "                      \"I dont like him.\"])\n",
        "\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "Qns0HAK9QH51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Deeper in Outputs"
      ],
      "metadata": {
        "id": "QksH89U3R8ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "txt = \"We are very happy to show you the Transformers library.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tokenizer(txt) # add first token\n",
        "\n",
        "\n",
        "print (f' Tokens: {tokens}')\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "print (f'Token IDs: {token_ids}')\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "print (f'Input IDs: {input_ids}')"
      ],
      "metadata": {
        "id": "VYG4VWa6RXdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [\"We are very happy to show you the Transformers library.\",\n",
        "            \"We hope you don't hate it.\"]\n",
        "\n",
        "batch = tokenizer(X_train, padding=True, truncation=True,\n",
        "                   max_length=512, return_tensors=\"pt\") # apply padding for smaller sentence tokens\n",
        "\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "d1ISuuZkSiBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Deep in Pytorch Model"
      ],
      "metadata": {
        "id": "jkLW877CT5gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "X_train = [\"We are very happy to show you the Transformers library.\",\n",
        "            \"We hope you don't hate it.\"]\n",
        "\n",
        "batch = tokenizer(X_train, padding=True, truncation=True,\n",
        "                   max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Input Batch: \", batch)\n",
        "print(\"\\n \\n\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "    print(\"Outputs: \", outputs)\n",
        "    print(\"\\n \\n\")\n",
        "\n",
        "    predictions = F.softmax(outputs.logits, dim=1)\n",
        "    print(\"Predictions: \", predictions)\n",
        "    print(\"\\n \\n\")\n",
        "\n",
        "    labels = torch.argmax(predictions, dim=1) # get indice of max value\n",
        "    print(\"Labels: \", labels)\n",
        "    print(\"\\n \\n\")\n",
        "\n",
        "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
        "    print(\"Final Label: \", labels)"
      ],
      "metadata": {
        "id": "YJHUlqx4S0kx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}