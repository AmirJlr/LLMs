{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlAq03beAylZ+pzUcRUQBa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirJlr/LLMs/blob/master/05_Simple_RAG_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qOLT4q9YvVUH",
        "outputId": "0add3c38-c201-47fc-aa86-cb0b069462e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary packages with a specific version for huggingface_hub to ensure compatibility\n",
        "!pip install -q langchain langchain-community langchain-google-genai langgraph transformers sentence-transformers huggingface-hub bitsandbytes accelerate faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "N_hHYcLKCzH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple RAG"
      ],
      "metadata": {
        "id": "cCRAhsXBR5gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Set hugging face READ access token and store it in colab secrets\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Securely get the Hugging Face API token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
        "print(\"Token set successfully!\")"
      ],
      "metadata": {
        "id": "Mh-PetlDvohf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for our data\n",
        "!mkdir -p documents\n",
        "\n",
        "# Create a dummy text file with some notes\n",
        "file_content = \"\"\"\n",
        "Real Madrid named Forbes' most valuable football club in the world for the fourth successive year\n",
        "Our club tops the list with a value of 6.75 billion dollars, and is the first football team to eclipse 1 billion dollars in revenues.\n",
        "\n",
        "\n",
        "Real Madrid named Forbes' most valuable football club in the world for the fourth successive year\n",
        "NEWS.31/05/2025\n",
        "Real Madrid has been named the most valuable football club in the world for the fourth consecutive year, and the ninth time in the last twelve editions of the list drawn up by Forbes, which values the club at 6.75 billion dollars.\n",
        "They're followed by Manchester United (6.6 billion dollars). The prestigious publication has released its annual report, which reveals our club's value has risen 2% compared to last year.\n",
        "\n",
        "Forbes highlights the fact that Real Madrid enjoyed revenues of 1.13 billion dollars in the 2023/24 season, making them the first football team ever to break the 1 billion dollar mark.\n",
        "The magazine also underlines Real Madrid's Champions League win in 2023/24, as well as the club's international appeal and commercial partnerships.\n",
        "Furthermore, they highlight how, following the completion of the Santiago BernabÃ©u, the club expects to increase its matchday revenues, including in ticket sales.\n",
        "\n",
        "\n",
        "POSITION\t                  CLUB\tVALUE  IN BILLIONS OF DOLLARS\n",
        "1\tReal Madrid\t                         6.75\n",
        "2\tManchester United\t                 6.60\n",
        "3\tF. C. Barcelona\t                     5.65\n",
        "4\tLiverpool\t                         5.40\n",
        "5\tManchester City\t                     5.30\n",
        "6\tBayern Munich\t                     5.10\n",
        "7\tPSG\t                                 4.60\n",
        "8\tArsenal\t                             3.40\n",
        "9\tTottenham\t                         3.30\n",
        "10\tChelsea\t                             3.25\n",
        "\"\"\"\n",
        "\n",
        "with open(\"documents/ReadMadrid.txt\", \"w\") as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print(\"Sample file 'ReadMadrid.txt' created.\")"
      ],
      "metadata": {
        "id": "CLPDSFBKx-k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and Chunk Documents"
      ],
      "metadata": {
        "id": "-AudZJl80TXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load documents from directory (you can add your text files here)\n",
        "def load_documents(directory_path=\"./documents\"):\n",
        "    \"\"\"Load all text files from directory\"\"\"\n",
        "    if not os.path.exists(directory_path):\n",
        "        os.makedirs(directory_path)\n",
        "        print(f\"Created {directory_path} directory. Add your text files here!\")\n",
        "        return []\n",
        "\n",
        "    loader = DirectoryLoader(directory_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    return loader.load()"
      ],
      "metadata": {
        "id": "70D3PBSZDd1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Split documents into chunks\n",
        "def create_chunks(documents):\n",
        "    \"\"\"Split documents into smaller chunks for better retrieval\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    return text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "-3Q0Xu_cDpM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Create Vector Embeddings and Store in FAISS"
      ],
      "metadata": {
        "id": "w787ckxr1dby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create vector store with embeddings\n",
        "def create_vector_store(chunks):\n",
        "    \"\"\"Create FAISS vector store from document chunks\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "    return FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "NXplvGi31FaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Create the Q&A Chain"
      ],
      "metadata": {
        "id": "3nuXXwzY2G5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Setup LLM and QA chain\n",
        "def setup_qa_chain(vector_store):\n",
        "    \"\"\"Setup the question-answering chain\"\"\"\n",
        "    # Use a lightweight model for quick testing\n",
        "    llm_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"allenai/unifiedqa-t5-base\",\n",
        "        max_length=512,\n",
        "        temperature=0.3,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "    # Create retrieval QA chain\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 1}),\n",
        "    )"
      ],
      "metadata": {
        "id": "3-ujkINv12Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Main chatbot function"
      ],
      "metadata": {
        "id": "istDS7q63a6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"ğŸ§  Loading your documents...\")\n",
        "    documents = load_documents()\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No documents found! Add .txt files to ./documents directory\")\n",
        "        return\n",
        "\n",
        "    print(\"ğŸ“„ Creating document chunks...\")\n",
        "    chunks = create_chunks(documents)\n",
        "\n",
        "    print(\"ğŸ” Building vector store...\")\n",
        "    vector_store = create_vector_store(chunks)\n",
        "\n",
        "    print(\"ğŸ¤– Setting up Q&A chain...\")\n",
        "    qa_chain = setup_qa_chain(vector_store)\n",
        "\n",
        "    print(\"\\nâœ… Chatbot ready! Type 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nâ“ Ask a question: \")\n",
        "        if question.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        answer = qa_chain.run(question)\n",
        "        print(f\"ğŸ¤– Answer: {answer}\")"
      ],
      "metadata": {
        "id": "FNIgPcvi3YXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "0h2sJMRjAb4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG LangGraph"
      ],
      "metadata": {
        "id": "ybGBdxReSB7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider = \"google_genai\")"
      ],
      "metadata": {
        "id": "anmp8GHTEtdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}