{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model with DPO\n",
        "\n",
        "Code authored by: Shaw Talebi\n",
        "\n",
        "https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/dpo"
      ],
      "metadata": {
        "id": "fad57205-9839-4c40-b791-03d60cdc21f5"
      },
      "id": "fad57205-9839-4c40-b791-03d60cdc21f5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "70487db3-18d6-4e79-93fc-905e714f0cd5"
      },
      "id": "70487db3-18d6-4e79-93fc-905e714f0cd5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps git+https://github.com/lvwerra/trl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUbAJWscLX0u",
        "outputId": "121ce6ef-f98d-4389-fd77-d8ea7bb101f8"
      },
      "id": "FUbAJWscLX0u",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/lvwerra/trl.git\n",
            "  Cloning https://github.com/lvwerra/trl.git to /tmp/pip-req-build-ktnjsoj9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-ktnjsoj9\n",
            "  Resolved https://github.com/lvwerra/trl.git to commit 722847abbc9de2a6a34d11bc0f416239f85ec1d6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: trl\n",
            "  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.19.0-py3-none-any.whl size=366346 sha256=e366cb4caeac8e8be61a53795bfe84b7e0edf2c7a2a91c612894f54216eb18ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2d3p88ys/wheels/76/66/fa/d68bc139d65382faf9238f75aab5bf48a9d50a100b18575691\n",
            "Successfully built trl\n",
            "Installing collected packages: trl\n",
            "Successfully installed trl-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTfdFzEqN0-v",
        "outputId": "97b75755-6c2f-455b-e492-f0cc4e89a8fc"
      },
      "id": "UTfdFzEqN0-v",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "d6b288de-ec97-4cda-bba2-d35fcbc62f8b"
      },
      "id": "d6b288de-ec97-4cda-bba2-d35fcbc62f8b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load data"
      ],
      "metadata": {
        "id": "897ecf2d-3ae3-4d6b-9c9d-273f51dbcc21"
      },
      "id": "897ecf2d-3ae3-4d6b-9c9d-273f51dbcc21"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"shawhin/youtube-titles-dpo\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb6e848-eaee-4fd7-8a86-8dc1dcc87711",
        "outputId": "45a21d86-bbe7-4843-8d67-22eb78708500"
      },
      "id": "5bb6e848-eaee-4fd7-8a86-8dc1dcc87711"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H063haEeST8Y",
        "outputId": "cd222445-5253-4eca-8784-c1c4d95e9185"
      },
      "id": "H063haEeST8Y",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'chosen', 'rejected'],\n",
              "        num_rows: 1026\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['prompt', 'chosen', 'rejected'],\n",
              "        num_rows: 114\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model"
      ],
      "metadata": {
        "id": "558119ea-32a7-4125-84f6-20463821ea3d"
      },
      "id": "558119ea-32a7-4125-84f6-20463821ea3d"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # set pad token"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "a6e7c0fb-7b79-4b1f-8859-e822f087da21"
      },
      "id": "a6e7c0fb-7b79-4b1f-8859-e822f087da21"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate title with base model"
      ],
      "metadata": {
        "id": "6ab8d520-91c9-4c2a-be2e-6b02cafc8418"
      },
      "id": "6ab8d520-91c9-4c2a-be2e-6b02cafc8418"
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(user_input, system_message=\"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Formats user input into the chat template format with <|im_start|> and <|im_end|> tags.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format user message\n",
        "    user_prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n\"\n",
        "\n",
        "    # Start assistant's turn\n",
        "    assistant_prompt = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Combine prompts\n",
        "    formatted_prompt = user_prompt + assistant_prompt\n",
        "\n",
        "    return formatted_prompt"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "7a2d42dc-7eca-4007-aa16-0eee1049f49d"
      },
      "id": "7a2d42dc-7eca-4007-aa16-0eee1049f49d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "Given the YouTube video idea write an engaging title.\n",
            "\n",
            "**Video Idea**: intro independent component analysis\n",
            "\n",
            "**Additional Guidance**:\n",
            "- Title should be between 30 and 75 characters long\n",
            "- Only return the title idea, nothing else!<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\"Unlocking the Secrets of Independent Component Analysis: A Step-by-Step Guide\"\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6535198b-d0ce-4e48-9681-14ed678d55ea",
        "outputId": "f4ebfeee-ca2f-45e4-fba1-d75571d95793"
      },
      "id": "6535198b-d0ce-4e48-9681-14ed678d55ea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model"
      ],
      "metadata": {
        "id": "bdd86db3-dd2b-4f4c-9716-3d5fd88075d8"
      },
      "id": "bdd86db3-dd2b-4f4c-9716-3d5fd88075d8"
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model_name = model_name.split('/')[1].replace(\"Instruct\", \"DPO\")\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=ft_model_name,\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    bf16=True,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    eval_steps=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "device = torch.device('cuda')"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "d6b4e8a7-8860-4726-8783-437e10072e3c"
      },
      "id": "d6b4e8a7-8860-4726-8783-437e10072e3c"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['valid'],\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "-jsywUCLfO5M",
        "outputId": "6b9225c9-2e6c-427b-c095-a2a1438cf737"
      },
      "id": "-jsywUCLfO5M",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='387' max='387' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [387/387 31:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rewards/chosen</th>\n",
              "      <th>Rewards/rejected</th>\n",
              "      <th>Rewards/accuracies</th>\n",
              "      <th>Rewards/margins</th>\n",
              "      <th>Logps/chosen</th>\n",
              "      <th>Logps/rejected</th>\n",
              "      <th>Logits/chosen</th>\n",
              "      <th>Logits/rejected</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.573100</td>\n",
              "      <td>0.553492</td>\n",
              "      <td>1.461951</td>\n",
              "      <td>0.936057</td>\n",
              "      <td>0.719298</td>\n",
              "      <td>0.525894</td>\n",
              "      <td>-41.500343</td>\n",
              "      <td>-50.369747</td>\n",
              "      <td>-3.442668</td>\n",
              "      <td>-3.443272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.415900</td>\n",
              "      <td>0.519218</td>\n",
              "      <td>0.631974</td>\n",
              "      <td>-0.386793</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>1.018767</td>\n",
              "      <td>-49.800110</td>\n",
              "      <td>-63.598251</td>\n",
              "      <td>-3.564236</td>\n",
              "      <td>-3.576619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.359000</td>\n",
              "      <td>0.547834</td>\n",
              "      <td>0.499795</td>\n",
              "      <td>-0.708028</td>\n",
              "      <td>0.692982</td>\n",
              "      <td>1.207822</td>\n",
              "      <td>-51.121899</td>\n",
              "      <td>-66.810593</td>\n",
              "      <td>-3.468524</td>\n",
              "      <td>-3.483689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=387, training_loss=0.4653558262866905, metrics={'train_runtime': 1886.9665, 'train_samples_per_second': 1.631, 'train_steps_per_second': 0.205, 'total_flos': 0.0, 'train_loss': 0.4653558262866905, 'epoch': 3.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### use fine-tuned model"
      ],
      "metadata": {
        "id": "cbf20be6-48b0-4ae2-96ec-92886bd72bdf"
      },
      "id": "cbf20be6-48b0-4ae2-96ec-92886bd72bdf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "ft_model = trainer.model"
      ],
      "metadata": {
        "id": "0gJBy3o-fRVh"
      },
      "id": "0gJBy3o-fRVh",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer, device='cuda')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "Given the YouTube video idea write an engaging title.\n",
            "\n",
            "**Video Idea**: intro independent component analysis\n",
            "\n",
            "**Additional Guidance**:\n",
            "- Title should be between 30 and 75 characters long\n",
            "- Only return the title idea, nothing else!<|im_end|>\n",
            "<|im_start|>assistant\n",
            "**Title Idea**: \"Independent Component Analysis: Exploring Unsupervised Signal Separation\"\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47134052-877c-4ec8-8c87-18a5827f7a31",
        "outputId": "5c84a85d-0bfd-43a5-9751-cfbaf0feaa81"
      },
      "id": "47134052-877c-4ec8-8c87-18a5827f7a31"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### push to HF hub"
      ],
      "metadata": {
        "id": "286014ed-2537-4bfa-ba7c-18616fbf87f5"
      },
      "id": "286014ed-2537-4bfa-ba7c-18616fbf87f5"
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = f\"shawhin/{ft_model_name}\"\n",
        "trainer.push_to_hub(model_id)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "667b175bffd8450a99bbf35d89b10ba3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "training_args.bin:   0%|          | 0.00/6.20k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81d794ee11134a418c13d1b109bee526",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9d632df7ace401088b8314553537ff0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/shawhin/Qwen2.5-0.5B-DPO/commit/4f36e42b1a38ef4504cd2cde6f9799ad0ef8a36d', commit_message='shawhin/Qwen2.5-0.5B-DPO', commit_description='', oid='4f36e42b1a38ef4504cd2cde6f9799ad0ef8a36d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shawhin/Qwen2.5-0.5B-DPO', endpoint='https://huggingface.co', repo_type='model', repo_id='shawhin/Qwen2.5-0.5B-DPO'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "667b175bffd8450a99bbf35d89b10ba3",
            "81d794ee11134a418c13d1b109bee526",
            "e9d632df7ace401088b8314553537ff0"
          ]
        },
        "id": "c3a08d5e-939e-4b1d-ad6b-f8548b001c30",
        "outputId": "d79a0d42-6ca0-4c3b-8884-534ffa82409b"
      },
      "id": "c3a08d5e-939e-4b1d-ad6b-f8548b001c30"
    },
    {
      "cell_type": "code",
      "source": [
        "format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": null,
          "data": {
            "text/plain": "'<|im_start|>user\\nGiven the YouTube video idea write an engaging title.\\n\\n**Video Idea**: intro independent component analysis\\n\\n**Additional Guidance**:\\n- Title should be between 30 and 75 characters long\\n- Only return the title idea, nothing else!<|im_end|>\\n<|im_start|>assistant\\n'"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "1e30762d-0b15-43b4-8657-b1f70d40194b",
        "outputId": "80de0d37-f726-4dfe-a6cd-aaa726ecf847"
      },
      "id": "1e30762d-0b15-43b4-8657-b1f70d40194b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "e1d05e2c-1324-43c8-a67c-e3268261c6b6"
      },
      "id": "e1d05e2c-1324-43c8-a67c-e3268261c6b6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "nteract": {
      "version": "0.28.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
